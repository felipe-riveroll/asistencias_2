# ğŸ§ª Pruebas Unitarias - Sistema de Reportes de Asistencia

Este documento explica cÃ³mo ejecutar y entender las pruebas unitarias del sistema de generaciÃ³n de reportes de asistencia.

## ğŸ“ **Estructura del Proyecto**

```
nuevo_asistencias/
â”œâ”€â”€ ğŸ“ tests/                                    # Carpeta de pruebas
â”‚   â”œâ”€â”€ __init__.py                             # Paquete Python
â”‚   â”œâ”€â”€ test_generar_reporte_optimizado.py      # Pruebas bÃ¡sicas (31 tests)
â”‚   â”œâ”€â”€ test_casos_edge.py                      # Casos edge (34 tests)
â”‚   â””â”€â”€ run_tests.py                            # Ejecutor interno
â”œâ”€â”€ ğŸ“„ pyproject.toml                           # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ ğŸ“„ pytest.ini                               # ConfiguraciÃ³n pytest
â”œâ”€â”€ ğŸ“„ run_tests.py                             # Ejecutor principal
â”œâ”€â”€ ğŸ“„ generar_reporte_optimizado.py            # Script principal
â””â”€â”€ ğŸ“„ db_postgres_connection.py                # ConexiÃ³n BD
```

## ğŸš€ **CÃ³mo Ejecutar las Pruebas**

### **Comandos Principales (Desde la raÃ­z del proyecto):**

```bash
# Ver resumen de pruebas disponibles
python run_tests.py summary

# Ejecutar todas las pruebas (65 tests)
python run_tests.py all

# Ejecutar solo pruebas bÃ¡sicas (31 tests)
python run_tests.py basic

# Ejecutar solo casos edge (34 tests)
python run_tests.py edge

# Ejecutar con cobertura de cÃ³digo
python run_tests.py coverage

# Ejecutar pruebas rÃ¡pidas (sin tests lentos)
python run_tests.py fast
```

### **Comandos Directos con Pytest:**

```bash
# Ejecutar todas las pruebas
uv run python -m pytest tests/ -v

# Ejecutar archivo especÃ­fico
uv run python -m pytest tests/test_generar_reporte_optimizado.py -v

# Ejecutar clase especÃ­fica
uv run python -m pytest tests/test_generar_reporte_optimizado.py::TestGenerarReporteOptimizado -v

# Ejecutar prueba especÃ­fica
uv run python -m pytest tests/test_generar_reporte_optimizado.py::TestGenerarReporteOptimizado::test_obtener_codigos_empleados_api -v

# Ejecutar con marcadores
uv run python -m pytest tests/ -m "not slow" -v
uv run python -m pytest tests/ -m "unit" -v
uv run python -m pytest tests/ -m "edge" -v
```

## ğŸ“Š **Resumen de Pruebas**

### **âœ… Pruebas BÃ¡sicas (31 tests)**
- **Archivo:** `tests/test_generar_reporte_optimizado.py`
- **Funciones probadas:**
  - `obtener_codigos_empleados_api()`
  - `process_checkins_to_dataframe()`
  - `calcular_proximidad_horario()`
  - `procesar_horarios_con_medianoche()`
  - `analizar_asistencia_con_horarios_cache()`
  - `generar_resumen_periodo()`
  - `fetch_checkins()`

### **âœ… Casos Edge (34 tests)**
- **Archivo:** `tests/test_casos_edge.py`
- **Tipos de pruebas:**
  - ValidaciÃ³n de formatos de hora
  - Casos lÃ­mite (medianoche)
  - Datos nulos e invÃ¡lidos
  - MÃºltiples checadas por dÃ­a
  - Turnos nocturnos
  - Fechas extremas

### **ğŸ“ˆ Resultados Esperados:**
```
============================= 65 passed in 1.20s =============================
ğŸ¯ Resultado todas las pruebas: âœ… PASARON
```

## ğŸ§ª **Tipos de Pruebas Implementadas**

### **1. Pruebas Unitarias BÃ¡sicas**
```python
def test_obtener_codigos_empleados_api(checkin_data):
    """Prueba la extracciÃ³n de cÃ³digos de empleados."""
    codigos = obtener_codigos_empleados_api(checkin_data)
    assert len(codigos) == 2
    assert "EMP001" in codigos
    assert "EMP002" in codigos
```

### **2. Pruebas Parametrizadas**
```python
@pytest.mark.parametrize("checada,hora_prog,esperado", [
    ("08:00:00", "08:00", 0),      # A tiempo
    ("08:15:00", "08:00", 15),     # 15 min tarde
    ("07:45:00", "08:00", 15),     # 15 min temprano
    ("09:00:00", "08:00", 60),     # 1 hora tarde
])
def test_calcular_proximidad_horario_parametrizado(checada, hora_prog, esperado):
    resultado = calcular_proximidad_horario(checada, hora_prog)
    assert resultado == esperado
```

### **3. Pruebas con Fixtures (Datos Reutilizables)**
```python
@pytest.fixture
def cache_horarios():
    """Datos de horarios para las pruebas."""
    return {
        "EMP001": {
            1: {"hora_entrada": "08:00", "hora_salida": "17:00", "cruza_medianoche": False},
            2: {"hora_entrada": "08:00", "hora_salida": "17:00", "cruza_medianoche": False},
            3: {"hora_entrada": "08:00", "hora_salida": "17:00", "cruza_medianoche": False}
        }
    }

def test_analizar_asistencia_con_horarios_cache(cache_horarios):
    # Los datos se inyectan automÃ¡ticamente
    df_test = create_test_dataframe()
    resultado = analizar_asistencia_con_horarios_cache(df_test, cache_horarios)
    assert not resultado.empty
```

### **4. Pruebas con Mocking (SimulaciÃ³n de APIs)**
```python
@patch('requests.get')
def test_fetch_checkins_success(mock_get):
    """Prueba la obtenciÃ³n de checadas desde la API."""
    # Simular respuesta de la API
    mock_response = MagicMock()
    mock_response.json.return_value = {
        "data": [
            {"employee": "EMP001", "time": "2025-01-15 08:00:00"}
        ]
    }
    mock_get.return_value = mock_response
    
    # Ejecutar funciÃ³n
    result = fetch_checkins("2025-01-15", "2025-01-15", "token")
    
    # Verificar resultado
    assert len(result) == 1
    assert result[0]["employee"] == "EMP001"
```

### **5. Pruebas de IntegraciÃ³n**
```python
def test_flujo_completo_analisis(checkin_data_integracion, cache_horarios_integracion):
    """Prueba el flujo completo de anÃ¡lisis."""
    # 1. Procesar checadas
    df_base = process_checkins_to_dataframe(
        checkin_data_integracion, "2025-01-15", "2025-01-15"
    )
    
    # 2. Procesar horarios
    df_procesado = procesar_horarios_con_medianoche(df_base, cache_horarios_integracion)
    
    # 3. Analizar asistencia
    df_analizado = analizar_asistencia_con_horarios_cache(
        df_procesado, cache_horarios_integracion
    )
    
    # Verificar resultado final
    assert not df_analizado.empty
    assert "estado_asistencia" in df_analizado.columns
```

## ğŸ¯ **Casos de Prueba EspecÃ­ficos**

### **AnÃ¡lisis de Retardos (Parametrizado)**
```python
@pytest.mark.parametrize("checada,hora_prog,esperado", [
    ("08:00:00", "08:00", "A Tiempo"),           # Exacto
    ("08:15:00", "08:00", "A Tiempo"),           # Tolerancia 15 min
    ("08:16:00", "08:00", "Retardo"),            # Retardo leve
    ("08:30:00", "08:00", "Retardo"),            # Retardo moderado
    ("08:31:00", "08:00", "Falta Injustificada"), # Falta
    (None, "08:00", "Falta"),                    # Sin checada
])
def test_analizar_retardo_casos_especificos_parametrizado(checada, hora_prog, esperado):
    # Prueba todos los casos de clasificaciÃ³n de asistencia
```

### **ValidaciÃ³n de Formatos de Hora**
```python
@pytest.mark.parametrize("checada,hora_prog,esperado", [
    ("08:00:00", "08:00", 0),        # Formato vÃ¡lido
    ("8:0:0", "08:00", 0),           # Formato vÃ¡lido (se puede parsear)
    ("08:00", "8:0", float('inf')),  # Formato invÃ¡lido
    ("25:00:00", "08:00", float('inf')), # Hora invÃ¡lida
    ("08:60:00", "08:00", float('inf')), # Minuto invÃ¡lido
])
def test_validacion_formato_horas(checada, hora_prog, esperado):
    resultado = calcular_proximidad_horario(checada, hora_prog)
    assert resultado == esperado
```

### **Casos Edge con Turnos Nocturnos**
```python
@pytest.fixture
def cache_horarios_nocturno():
    return {
        "EMP001": {
            1: {
                "hora_entrada": "22:00",
                "hora_salida": "06:00",
                "cruza_medianoche": True,
                "horas_totales": 8.0
            }
        }
    }

def test_analizar_asistencia_turno_nocturno_casos_edge(cache_horarios_nocturno):
    """Pruebas especÃ­ficas para turnos que cruzan medianoche."""
    # Pruebas con checadas antes y despuÃ©s de medianoche
```

## ğŸ”§ **ConfiguraciÃ³n del Proyecto**

### **pyproject.toml**
```toml
[project]
name = "nuevo-asistencias"
version = "1.0.0"
description = "Sistema de reportes de asistencia optimizado"
requires-python = ">=3.8"

dependencies = [
    "pandas>=1.5.0",
    "numpy>=1.21.0",
    "requests>=2.28.0",
    "python-dotenv>=0.19.0",
    "psycopg2-binary>=2.9.0",
]

[project.optional-dependencies]
test = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-mock>=3.8.0",
]
```

### **pytest.ini**
```ini
[tool:pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--disable-warnings",
    "--color=yes",
]
markers = [
    "slow: marks tests as slow",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "edge: marks tests as edge case tests",
]
```

## ğŸ“ˆ **Cobertura de CÃ³digo**

### **Ejecutar con Cobertura:**
```bash
# Instalar pytest-cov si no estÃ¡ instalado
uv add pytest-cov

# Ejecutar con cobertura
python run_tests.py coverage

# O directamente
uv run python -m pytest tests/ --cov=generar_reporte_optimizado --cov-report=term-missing --cov-report=html
```

### **Reporte de Cobertura:**
```
Name                                    Stmts   Miss  Cover   Missing
-------------------------------------------------------------------------
generar_reporte_optimizado.py             156      5    97%   45, 67, 89, 123, 145
-------------------------------------------------------------------------
TOTAL                                     156      5    97%
```

## ğŸš¨ **SoluciÃ³n de Problemas**

### **Error: "No module named 'generar_reporte_optimizado'"**
```bash
# AsegÃºrate de estar en la raÃ­z del proyecto
cd /ruta/a/nuevo_asistencias

# Verifica que el archivo existe
ls generar_reporte_optimizado.py
```

### **Error: "pytest command not found"**
```bash
# Instalar pytest
uv add pytest

# O usar uv run
uv run python -m pytest tests/ -v
```

### **Error: "ImportError in tests"**
```bash
# Los tests estÃ¡n configurados para agregar automÃ¡ticamente el directorio padre al path
# Si persiste el error, verifica que estÃ¡s ejecutando desde la raÃ­z del proyecto
```

## ğŸ¯ **Marcadores de Pruebas**

### **Marcadores Disponibles:**
- `@pytest.mark.slow` - Pruebas que tardan mÃ¡s tiempo
- `@pytest.mark.integration` - Pruebas de integraciÃ³n
- `@pytest.mark.unit` - Pruebas unitarias
- `@pytest.mark.edge` - Casos edge
- `@pytest.mark.api` - Pruebas que interactÃºan con APIs
- `@pytest.mark.database` - Pruebas que usan base de datos

### **Ejecutar por Marcadores:**
```bash
# Solo pruebas rÃ¡pidas
uv run python -m pytest tests/ -m "not slow" -v

# Solo pruebas unitarias
uv run python -m pytest tests/ -m "unit" -v

# Solo casos edge
uv run python -m pytest tests/ -m "edge" -v
```

## ğŸ“ **Agregar Nuevas Pruebas**

### **1. Crear nueva prueba bÃ¡sica:**
```python
# En tests/test_generar_reporte_optimizado.py
def test_nueva_funcionalidad():
    """Prueba para nueva funcionalidad."""
    # Arrange
    datos_entrada = [...]
    
    # Act
    resultado = funcion_a_probar(datos_entrada)
    
    # Assert
    assert resultado == esperado
```

### **2. Crear nueva prueba parametrizada:**
```python
@pytest.mark.parametrize("entrada,esperado", [
    ("caso1", "resultado1"),
    ("caso2", "resultado2"),
])
def test_funcion_parametrizada(entrada, esperado):
    resultado = funcion_a_probar(entrada)
    assert resultado == esperado
```

### **3. Crear nuevo fixture:**
```python
@pytest.fixture
def datos_especiales():
    """Fixture para datos especiales."""
    return {
        "datos": [...],
        "configuracion": {...}
    }

def test_con_datos_especiales(datos_especiales):
    # Los datos se inyectan automÃ¡ticamente
    pass
```

## ğŸ‰ **Resumen**

### **âœ… Estado Actual:**
- **65 pruebas** implementadas y pasando
- **Tiempo de ejecuciÃ³n:** ~1.20 segundos
- **Cobertura de cÃ³digo:** ~97%
- **Estructura profesional** con carpeta `tests/`
- **ConfiguraciÃ³n moderna** con `pyproject.toml`

### **ğŸ¯ Comandos Principales:**
```bash
# Verificar estado
python run_tests.py summary

# Ejecutar todas las pruebas
python run_tests.py all

# Ejecutar con cobertura
python run_tests.py coverage
```

### **ğŸ“Š Funciones Cubiertas:**
- âœ… `obtener_codigos_empleados_api()`
- âœ… `process_checkins_to_dataframe()`
- âœ… `calcular_proximidad_horario()`
- âœ… `procesar_horarios_con_medianoche()`
- âœ… `analizar_asistencia_con_horarios_cache()`
- âœ… `generar_resumen_periodo()`
- âœ… `fetch_checkins()`

**Â¡El sistema de pruebas estÃ¡ completamente funcional y listo para uso!** ğŸš€ 