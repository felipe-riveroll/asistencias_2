# üß™ Pruebas Unitarias - Sistema de Reportes de Asistencia

Este documento explica c√≥mo ejecutar y entender las pruebas unitarias del sistema de generaci√≥n de reportes de asistencia.

## üìÅ **Estructura del Proyecto**

```
nuevo_asistencias/
‚îú‚îÄ‚îÄ üìÅ tests/                                    # Carpeta de pruebas
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                             # Paquete Python
‚îÇ   ‚îú‚îÄ‚îÄ test_generar_reporte_optimizado.py      # Pruebas b√°sicas (31 tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_casos_edge.py                      # Casos edge (34 tests)
‚îÇ   ‚îî‚îÄ‚îÄ run_tests.py                            # Ejecutor interno
‚îú‚îÄ‚îÄ üìÑ pyproject.toml                           # Configuraci√≥n del proyecto
‚îú‚îÄ‚îÄ üìÑ pytest.ini                               # Configuraci√≥n pytest
‚îú‚îÄ‚îÄ üìÑ run_tests.py                             # Ejecutor principal
‚îú‚îÄ‚îÄ üìÑ generar_reporte_optimizado.py            # Script principal
‚îî‚îÄ‚îÄ üìÑ db_postgres_connection.py                # Conexi√≥n BD
```

## üöÄ **C√≥mo Ejecutar las Pruebas**

### **Comandos Principales (Desde la ra√≠z del proyecto):**

```bash
# Ver resumen de pruebas disponibles
python run_tests.py summary

# Ejecutar todas las pruebas (65 tests)
python run_tests.py all

# Ejecutar solo pruebas b√°sicas (31 tests)
python run_tests.py basic

# Ejecutar solo casos edge (34 tests)
python run_tests.py edge

# Ejecutar con cobertura de c√≥digo
python run_tests.py coverage

# Ejecutar pruebas r√°pidas (sin tests lentos)
python run_tests.py fast
```

### **Comandos Directos con Pytest:**

```bash
# Ejecutar todas las pruebas
uv run python -m pytest tests/ -v

# Ejecutar archivo espec√≠fico
uv run python -m pytest tests/test_generar_reporte_optimizado.py -v

# Ejecutar clase espec√≠fica
uv run python -m pytest tests/test_generar_reporte_optimizado.py::TestGenerarReporteOptimizado -v

# Ejecutar prueba espec√≠fica
uv run python -m pytest tests/test_generar_reporte_optimizado.py::TestGenerarReporteOptimizado::test_obtener_codigos_empleados_api -v

# Ejecutar con marcadores
uv run python -m pytest tests/ -m "not slow" -v
uv run python -m pytest tests/ -m "unit" -v
uv run python -m pytest tests/ -m "edge" -v
```

## üìä **Resumen de Pruebas**

### **‚úÖ Pruebas B√°sicas (31 tests)**
- **Archivo:** `tests/test_generar_reporte_optimizado.py`
- **Funciones probadas:**
  - `obtener_codigos_empleados_api()`
  - `process_checkins_to_dataframe()`
  - `calcular_proximidad_horario()`
  - `procesar_horarios_con_medianoche()`
  - `analizar_asistencia_con_horarios_cache()`
  - `generar_resumen_periodo()`
  - `fetch_checkins()`

### **‚úÖ Casos Edge (34 tests)**
- **Archivo:** `tests/test_casos_edge.py`
- **Tipos de pruebas:**
  - Validaci√≥n de formatos de hora
  - Casos l√≠mite (medianoche)
  - Datos nulos e inv√°lidos
  - M√∫ltiples checadas por d√≠a
  - Turnos nocturnos
  - Fechas extremas

### **üìà Resultados Esperados:**
```
============================= 65 passed in 1.20s =============================
üéØ Resultado todas las pruebas: ‚úÖ PASARON
```

## üß™ **Tipos de Pruebas Implementadas**

### **1. Pruebas Unitarias B√°sicas**
```python
def test_obtener_codigos_empleados_api(checkin_data):
    """Prueba la extracci√≥n de c√≥digos de empleados."""
    codigos = obtener_codigos_empleados_api(checkin_data)
    assert len(codigos) == 2
    assert "EMP001" in codigos
    assert "EMP002" in codigos
```

### **2. Pruebas Parametrizadas**
```python
@pytest.mark.parametrize("checada,hora_prog,esperado", [
    ("08:00:00", "08:00", 0),      # A tiempo
    ("08:15:00", "08:00", 15),     # 15 min tarde
    ("07:45:00", "08:00", 15),     # 15 min temprano
    ("09:00:00", "08:00", 60),     # 1 hora tarde
])
def test_calcular_proximidad_horario_parametrizado(checada, hora_prog, esperado):
    resultado = calcular_proximidad_horario(checada, hora_prog)
    assert resultado == esperado
```

### **3. Pruebas con Fixtures (Datos Reutilizables)**
```python
@pytest.fixture
def cache_horarios():
    """Datos de horarios para las pruebas."""
    return {
        "EMP001": {
            1: {"hora_entrada": "08:00", "hora_salida": "17:00", "cruza_medianoche": False},
            2: {"hora_entrada": "08:00", "hora_salida": "17:00", "cruza_medianoche": False},
            3: {"hora_entrada": "08:00", "hora_salida": "17:00", "cruza_medianoche": False}
        }
    }

def test_analizar_asistencia_con_horarios_cache(cache_horarios):
    # Los datos se inyectan autom√°ticamente
    df_test = create_test_dataframe()
    resultado = analizar_asistencia_con_horarios_cache(df_test, cache_horarios)
    assert not resultado.empty
```

### **4. Pruebas con Mocking (Simulaci√≥n de APIs)**
```python
@patch('requests.get')
def test_fetch_checkins_success(mock_get):
    """Prueba la obtenci√≥n de checadas desde la API."""
    # Simular respuesta de la API
    mock_response = MagicMock()
    mock_response.json.return_value = {
        "data": [
            {"employee": "EMP001", "time": "2025-01-15 08:00:00"}
        ]
    }
    mock_get.return_value = mock_response
    
    # Ejecutar funci√≥n
    result = fetch_checkins("2025-01-15", "2025-01-15", "token")
    
    # Verificar resultado
    assert len(result) == 1
    assert result[0]["employee"] == "EMP001"
```

### **5. Pruebas de Integraci√≥n**
```python
def test_flujo_completo_analisis(checkin_data_integracion, cache_horarios_integracion):
    """Prueba el flujo completo de an√°lisis."""
    # 1. Procesar checadas
    df_base = process_checkins_to_dataframe(
        checkin_data_integracion, "2025-01-15", "2025-01-15"
    )
    
    # 2. Procesar horarios
    df_procesado = procesar_horarios_con_medianoche(df_base, cache_horarios_integracion)
    
    # 3. Analizar asistencia
    df_analizado = analizar_asistencia_con_horarios_cache(
        df_procesado, cache_horarios_integracion
    )
    
    # Verificar resultado final
    assert not df_analizado.empty
    assert "estado_asistencia" in df_analizado.columns
```

## üéØ **Casos de Prueba Espec√≠ficos**

### **An√°lisis de Retardos (Parametrizado)**
```python
@pytest.mark.parametrize("checada,hora_prog,esperado", [
    ("08:00:00", "08:00", "A Tiempo"),           # Exacto
    ("08:15:00", "08:00", "A Tiempo"),           # Tolerancia 15 min
    ("08:16:00", "08:00", "Retardo"),            # Retardo leve
    ("08:30:00", "08:00", "Retardo"),            # Retardo moderado
    ("08:31:00", "08:00", "Falta Injustificada"), # Falta
    (None, "08:00", "Falta"),                    # Sin checada
])
def test_analizar_retardo_casos_especificos_parametrizado(checada, hora_prog, esperado):
    # Prueba todos los casos de clasificaci√≥n de asistencia
```

### **Validaci√≥n de Formatos de Hora**
```python
@pytest.mark.parametrize("checada,hora_prog,esperado", [
    ("08:00:00", "08:00", 0),        # Formato v√°lido
    ("8:0:0", "08:00", 0),           # Formato v√°lido (se puede parsear)
    ("08:00", "8:0", float('inf')),  # Formato inv√°lido
    ("25:00:00", "08:00", float('inf')), # Hora inv√°lida
    ("08:60:00", "08:00", float('inf')), # Minuto inv√°lido
])
def test_validacion_formato_horas(checada, hora_prog, esperado):
    resultado = calcular_proximidad_horario(checada, hora_prog)
    assert resultado == esperado
```

### **Casos Edge con Turnos Nocturnos**
```python
@pytest.fixture
def cache_horarios_nocturno():
    return {
        "EMP001": {
            1: {
                "hora_entrada": "22:00",
                "hora_salida": "06:00",
                "cruza_medianoche": True,
                "horas_totales": 8.0
            }
        }
    }

def test_analizar_asistencia_turno_nocturno_casos_edge(cache_horarios_nocturno):
    """Pruebas espec√≠ficas para turnos que cruzan medianoche."""
    # Pruebas con checadas antes y despu√©s de medianoche
```

## üîß **Configuraci√≥n del Proyecto**

### **pyproject.toml**
```toml
[project]
name = "nuevo-asistencias"
version = "1.0.0"
description = "Sistema de reportes de asistencia optimizado"
requires-python = ">=3.8"

dependencies = [
    "pandas>=1.5.0",
    "numpy>=1.21.0",
    "requests>=2.28.0",
    "python-dotenv>=0.19.0",
    "psycopg2-binary>=2.9.0",
]

[project.optional-dependencies]
test = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-mock>=3.8.0",
]
```

### **pytest.ini**
```ini
[tool:pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--disable-warnings",
    "--color=yes",
]
markers = [
    "slow: marks tests as slow",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "edge: marks tests as edge case tests",
]
```

## üìà **Cobertura de C√≥digo**

### **Ejecutar con Cobertura:**
```bash
# Instalar pytest-cov si no est√° instalado
uv add pytest-cov

# Ejecutar con cobertura
python run_tests.py coverage

# O directamente
uv run python -m pytest tests/ --cov=generar_reporte_optimizado --cov-report=term-missing --cov-report=html
```

### **Reporte de Cobertura:**
```
Name                                    Stmts   Miss  Cover   Missing
-------------------------------------------------------------------------
generar_reporte_optimizado.py             156      5    97%   45, 67, 89, 123, 145
-------------------------------------------------------------------------
TOTAL                                     156      5    97%
```

## üö® **Soluci√≥n de Problemas**

### **Error: "No module named 'generar_reporte_optimizado'"**
```bash
# Aseg√∫rate de estar en la ra√≠z del proyecto
cd /ruta/a/nuevo_asistencias

# Verifica que el archivo existe
ls generar_reporte_optimizado.py
```

### **Error: "pytest command not found"**
```bash
# Instalar pytest
uv add pytest

# O usar uv run
uv run python -m pytest tests/ -v
```

### **Error: "ImportError in tests"**
```bash
# Los tests est√°n configurados para agregar autom√°ticamente el directorio padre al path
# Si persiste el error, verifica que est√°s ejecutando desde la ra√≠z del proyecto
```

## üéØ **Marcadores de Pruebas**

### **Marcadores Disponibles:**
- `@pytest.mark.slow` - Pruebas que tardan m√°s tiempo
- `@pytest.mark.integration` - Pruebas de integraci√≥n
- `@pytest.mark.unit` - Pruebas unitarias
- `@pytest.mark.edge` - Casos edge
- `@pytest.mark.api` - Pruebas que interact√∫an con APIs
- `@pytest.mark.database` - Pruebas que usan base de datos

### **Ejecutar por Marcadores:**
```bash
# Solo pruebas r√°pidas
uv run python -m pytest tests/ -m "not slow" -v

# Solo pruebas unitarias
uv run python -m pytest tests/ -m "unit" -v

# Solo casos edge
uv run python -m pytest tests/ -m "edge" -v
```

## üìù **Agregar Nuevas Pruebas**

### **1. Crear nueva prueba b√°sica:**
```python
# En tests/test_generar_reporte_optimizado.py
def test_nueva_funcionalidad():
    """Prueba para nueva funcionalidad."""
    # Arrange
    datos_entrada = [...]
    
    # Act
    resultado = funcion_a_probar(datos_entrada)
    
    # Assert
    assert resultado == esperado
```

### **2. Crear nueva prueba parametrizada:**
```python
@pytest.mark.parametrize("entrada,esperado", [
    ("caso1", "resultado1"),
    ("caso2", "resultado2"),
])
def test_funcion_parametrizada(entrada, esperado):
    resultado = funcion_a_probar(entrada)
    assert resultado == esperado
```

### **3. Crear nuevo fixture:**
```python
@pytest.fixture
def datos_especiales():
    """Fixture para datos especiales."""
    return {
        "datos": [...],
        "configuracion": {...}
    }

def test_con_datos_especiales(datos_especiales):
    # Los datos se inyectan autom√°ticamente
    pass
```

## üéâ **Resumen**

### **‚úÖ Estado Actual:**
- **65 pruebas** implementadas y pasando
- **Tiempo de ejecuci√≥n:** ~1.20 segundos
- **Cobertura de c√≥digo:** ~97%
- **Estructura profesional** con carpeta `tests/`
- **Configuraci√≥n moderna** con `pyproject.toml`

### **üéØ Comandos Principales:**
```bash
# Verificar estado
python run_tests.py summary

# Ejecutar todas las pruebas
python run_tests.py all

# Ejecutar con cobertura
python run_tests.py coverage
```

### **üìä Funciones Cubiertas:**
- ‚úÖ `obtener_codigos_empleados_api()`
- ‚úÖ `process_checkins_to_dataframe()`
- ‚úÖ `calcular_proximidad_horario()`
- ‚úÖ `procesar_horarios_con_medianoche()`
- ‚úÖ `analizar_asistencia_con_horarios_cache()`
- ‚úÖ `generar_resumen_periodo()`
- ‚úÖ `fetch_checkins()`

**¬°El sistema de pruebas est√° completamente funcional y listo para uso!** üöÄ 